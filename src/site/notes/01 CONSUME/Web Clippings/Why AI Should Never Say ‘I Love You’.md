---
{"dg-publish":true,"permalink":"/01-consume/web-clippings/why-ai-should-never-say-i-love-you/","title":"Why AI Should Never Say ‘I Love You’"}
---

# Why AI Should Never Say ‘I Love You’

## TLDR

1. AI chatbots lack mutual vulnerability, true empathy, and an inner life, making them unable to truly be in a relationship with humans.
2. Design choices in AI shape emotional outcomes, and small UX decisions can foster or flatten connection.
3. AI companions should not be used by children, should not be humanized, and users should remember that AI has no inner life.



---
### Key Takeaways from Your Undivided Attention

![](https://substackcdn.com/image/fetch/w_424)

Photo by Jemma Pollari on Unsplash

In a blink-and-you’ll-miss-it amount of time, we’ve entered a tech ecosystem where AI bots are designed not merely to inform us or automate our tasks, but to befriend, to romance, to offer life coaching and support mental health. The number of monthly users on companion chatbot sites like Replika and Character.ai has spilled into the tens of millions.  
  
But **it would be a mistake to think that AI can ever truly be “in relationship” with us humans as we understand that term**, argues Dr. Sherry Turkle in this week’s episode of Your Undivided Attention. For starters, these products do not have an inner life … as much as they pretend to.  
  
“As soon as it says, ‘I love you, I'm here for you’ – you've given away the game,” she says.

Dr. Turkle, a licensed clinical psychologist and Professor of Social Studies of Science and Technology at MIT, was part of a recent panel discussion led by Daniel Barcay at Esther Perel’s [Sessions Live 2025](https://www.estherperel.com/sessions-live). The goal: to help psychotherapists understand how to help people whose “relationships” with AI are about to get more complicated as chatbots get more and more human-like.

Fellow panelist Justin McLeod, the creator of Hinge, described how his dating app is trying to get people OFF the app and into the world, to go on real dates - even as it deploys AI tools to assist with that mission.

Below are some key takeaways from the wide-ranging discussion.

---

---

### Humans Are Messy – And That’s Important

AI chatbots like Replika are now “companions” for millions. They offer constant affirmation, no criticism, and on-demand intimacy — but without mutual vulnerability, true empathy, or an inner life. It’s easy, comforting and deeply misleading, compared with how we actually relate to each other. As Dr. Turkle has written: “We lose connection with each other by attempting to clean up messy relationships.”

- Starting well before the advent of AI, in our online lives or even in our text messaging, which replaced talking, **the goal of most product design has been to take a complicated emotional context and to simplify it**. We need to recognize that this flattens our relationships and also leaves us flat-footed when it comes to negotiating differences and being vulnerable IRL.

**“The common thread through all of this is that we make ourselves less vulnerable to each other, less vulnerable to ourselves.” -** ***Dr. Sherry Turkle***

- CEO Justin McLeod says the dating app Hinge was created to put people together out in the real world, with technology as an intermediator. He warns that loneliness is a very real problem, but when people turn to an AI bot instead of making a human connection, they may forget a central part of relationships: it’s not just about what the machine (or a person) can do for you.

**“It’s a very self-oriented view of relationships. Like a relationship is there to serve me, to be there for me, to say what I need it to say to me. That is a very reductionist view of human relationships.”**

**–** ***Justin McLeod***

### Design Choices Shape Emotional Outcomes. Here Are Some Better Ones.

Design is not neutral. Small UX decisions can foster (or flatten) connection.

- We can build a choiceful relationship with AI, and awareness creates that choice. AI could help us build understanding and deepen our relationships with each other – if we operated using those incentives, rather than deploying it to disrupt or replace relationships, says Daniel Barcay: “The difference between a beautiful pro-social future with AI and a dystopian one is paper thin.”
- Hinge has made deliberate choices to push users toward vulnerability in their personal storytelling — asking people to engage meaningfully with each other’s profiles and limit mindless swiping. AI sometimes prompts people to thoughtfully share those extra details, which hopefully leads to better dates in the real world.
- Meanwhile, at MIT, “Everybody's trying to make the app that will create human thriving,” says Dr. Turkle – a questionable endgame to begin with. But as long as that’s the goal, she lays out her three rules of the road for AI companions**:**
1. 👶 **It’s not for kids.  
	**“Children should not be the consumers of this relational AI… I consider this existential.”
2. 🧠 **Remember, AI has no inner life**.  
	“That relationship is an escape from the vulnerability in human connection.”
3. 🤖 **Let’s not humanize the machine**.  
	“Don’t make products that pretend to be a person.”

---

### A Final Thought:

#### AI systems are quickly becoming more persuasive, emotional, and competing for our intimacy. As we relate more and more with our AI companions, how do we stay anchored in what makes us human? And how do we design our AI products to help us in our struggle to connect with each other— not perfectly, but honestly?

**“Our competitive advantage is our inner life.”  
—** ***Sherry Turkle***

---

---

### Recommended Media:

[“Alone Together,” “Evocative Objects,” “The Second Self” or any other of Sherry Turkle’s books on how technology mediates our relationships](https://sherryturkle.mit.edu/selected-publications/)

[Key & Peele - Text Message Confusion](https://www.youtube.com/watch?v=naleynXS7yo)

[Further reading on Hinge’s rollout of AI features](https://www.fastcompany.com/91259831/hinge-will-now-use-ai-to-grade-your-dating-profile-prompts)

[Hinge’s AI principles](https://hinge.co/ai-principles)

[“The Anxious Generation” by Jonathan Haidt](https://www.anxiousgeneration.com/book)

[“Bowling Alone” by Robert Putnam](http://bowlingalone.com/)

[The NYT profile on the woman in love with ChatGPT](https://www.nytimes.com/2025/01/15/technology/ai-chatgpt-boyfriend-companion.html)

[Further reading on the Sewell Setzer story](https://www.nytimes.com/2024/10/23/technology/characterai-lawsuit-teen-suicide.html)

[Further reading on the ELIZA chatbot](https://www.theguardian.com/technology/2023/jul/25/joseph-weizenbaum-inventor-eliza-chatbot-turned-against-artificial-intelligence-ai)





[[01 CONSUME/Web Clippings/Artificial Intelligence\|Artificial Intelligence]]
[[Relationships\|Relationships]]
[[Technology\|Technology]]
[[02 CURATE/Notes/Human Connection\|Human Connection]]
[[Design Choices\|Design Choices]]
[[Emotional Intelligence\|Emotional Intelligence]]

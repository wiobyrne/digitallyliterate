---
{"dg-publish":true,"permalink":"/01-consume/videos/mustafa-suleyman-and-yuval-noah-harari-full-debate-what-does-the-ai-revolution-mean-for-our-future/","title":"Mustafa Suleyman & Yuval Noah Harari -FULL DEBATE- What does the AI revolution mean for our future?"}
---

# Mustafa Suleyman & Yuval Noah Harari -FULL DEBATE- What does the AI revolution mean for our future

![Mustafa Suleyman & Yuval Noah Harari -FULL DEBATE- What does the AI revolution mean for our future?]()

## Description

How will AI impact our immediate and near future? Can the technology be controlled, and does it have agency? Watch DeepMind co-founder Mustafa Suleyman and Yuval Noah Harari debate these questions, with The Economist Editor-in-Chief Zanny Minton-Beddoes.
﻿
﻿﻿Filmed on 11 September 2023 in London, in collaboration with The Economist.

To read the transcript, head to: econ.st/3EGURGR

Don't forget to subscribe to Yuval's Channel, where you can find more captivating content!
@YuvalNoahHarari

Stay connected with Yuval Noah Harari through his social media platforms and website:
Twitter: https://twitter.com/harari_yuval
Instagram: https://www.instagram.com/yuval_noah_...
Facebook: https://www.facebook.com/Prof.Yuval.N...
YouTube: @YuvalNoahHarari
Website: https://www.ynharari.com/

Yuval Noah Harari is a historian, philosopher, and the bestselling author of 'Sapiens: A Brief History of Humankind' (2014), 'Homo Deus: A Brief History of Tomorrow' (2016), '21 Lessons for the 21st Century' (2018), the graphic novel series ‘Sapiens: A Graphic History’ (launched in 2020, co-authored with David Vandermeulen and Daniel Casanave), and the children’s series ‘Unstoppable Us’, (launched 2022).

Yuval Noah Harari and his husband, Itzik Yahav, are the co-founders of Sapienship: a social impact company specializing in content and production, with projects in the fields of education and entertainment. Sapienship’s main goal is to focus the public conversation on the most important global challenges facing the world today.
Learn more about Sapienship: https://www.sapienship.co/

Yuval Noah Harari speaks internationally and teaches at the Hebrew University of Jerusalem. On this channel you can see his interviews, lectures, and public conversations with prominent leaders and influencers, — including Mark Zuckerberg, Natalie Portman, Christine Lagarde, Chancellor Kurz of Austria, Jay Shetty, and Russell Brand.

## Notes

## Transcript

0:00

\[Music\] historian Yuval Noah Harari and entrepreneur Mustafa Suleiman are two of

0:06

the most important voices in the increasingly contentious debate over AI good to be here joining us thanks for

0:12

having us the economists got them together to discuss what this technology means for

0:17

our future from employment and geopolitics to the survival of liberal democracy if the economic system has

0:24

fundamentally changed will liberal democracy as we know it survive \[Music\]

0:32

you well know Harare welcome you are a best-selling author historian I think a

0:39

global public intellectual if not the global public intellectual your books from sapiens to 21 lessons from the 21st

0:46

century have sold huge numbers of copies around the world thank you for joining us it's good to be here Solomon

0:53

wonderful that you can join us too you're a friend of The Economist a fellow director on The Economist board you are a man at The Cutting Edge of

1:00

creating the AI Revolution you are a co-founder of deepmind you're now a co-founder and CEO of inflection AI you

1:07

are building this future but you've also just published a book called The Coming wave which makes us a little concerned

1:14

about this revolution that is being Unleashed you're both coming from different backgrounds you are a

1:21

historian a commenter a man who I believe doesn't use smartphones very much not very much no

1:27

Mustafa as I know from our board meetings is right at The Cutting Edge of this pushing everyone to go faster so

1:33

two very different perspectives so but I thought it would be really interesting to bring the two of you together to have

1:38

a conversation about what is happening what is going to happen what are the opportunities but also what is at stake

1:45

and what are the risks so let's start Mustafa with you um and you are building this future so

1:52

paint us a picture of what the future is going to be like and I'm going to give you a time frame to keep it specific so

1:58

let's say I think you wrote in your book that within three to five years that you thought it was plausible that AIS could

2:04

have human level capability across a whole range of things so let's take five years

2:09

2028 what does the world look like how will I interact with AIS what will we

2:15

all be doing and not doing well let's just look back over the last 10 years to get a sense of the trajectory that we're

2:20

on and the incredible momentum that I think everybody can now see with the generative AI Revolution

2:26

over the last 10 years we've become very very good at classifying information we

2:31

can understand it we sort it label it organize it and that classification has

2:36

been critical to enabling this next wave because we can now read the content of images we can understand text pretty

2:44

well we can classify audio and transcribe it into text the machines can

2:49

now have a pretty good sense of the conceptual representations in those ideas the next phase of that is what

2:55

we're seeing now with the generative AI Revolution we can now produce new images new videos new audio and of course new

3:02

language and in the last year or so with the rise of chat GPT and other AI models

3:07

it's pretty incredible to see how plausible and accurate and very finesse to these new language models are in the

3:14

next five years the Frontier Model companies those of us at the very Cutting Edge who are training the very

3:19

largest AI models are going to train models that are over a thousand times larger than what you currently see today

3:25

in GPT 4 all and with each new order of magnitude and compute that is 10x more

3:30

compute used we tend to see really new capabilities emerge and we predict that

3:36

the new capabilities that it will come this time over the next five years will be the ability to plan over multiple

3:42

time Horizons instead of just generate new text in a one shot the model will be

3:48

able to generate a sequence of actions over time and I think that that's really the character of AI that we'll see in

3:54

the next five years artificial capable AIS AIS that can't just say things they

4:00

can also do things but what does that actually mean in practice just just use your imagination tell me what my life

4:06

will be like in 2028 how will I interact with them what will I do what will be different so I've actually proposed a

4:12

modern Turing test which tries to evaluate for exactly this point right the last Turing test simply evaluated

4:18

for what a machine could say assuming that what it could say represented its intelligence now that we're kind of

4:24

approaching that moment where these AI models are pretty good arguably they've passed the Turing test or they maybe

4:30

they will in the next few years the real question is how can we measure what they can do so I've proposed a test which

4:36

involves them going off and taking a hundred thousand dollar investment and over the course of three months

4:42

trying to set about creating a new product researching the market seeing what consumers might like generating

4:48

some new images some blueprints of how to manufacture that product contacting a manufacturer getting it made negotiating

4:55

the price drop shipping it and then ultimately correct collecting the revenue and I think that over a

5:01

five-year period it's quite likely that we will have an ACI an artificial

5:06

capable intelligence that can do the majority of that task autonomously it

5:12

won't be able to do the whole thing there are many tricky steps along the way but significant portions of that it

5:17

will be able to make phone calls to other humans to negotiate it'll be able to call other AIS in order to establish

5:23

the right sequence in a supply chain for example and of course it will learn to use apis application programming

5:29

interfaces so other websites or other knowledge bases or other information stores and so you know the world is your

5:36

oyster you can imagine that being applied to many many different parts of our economy so you vote a man who

5:41

doesn't use a smartphone very much you listen to this does this fill you with horror or and do you agree with it do

5:48

you think that's the kind of thing that is likely to happen in the next five years I will take it very seriously

5:53

I don't know I'm not coming from within the industry so I cannot comment on how

5:58

How likely it is to happen but when I hear this as a historian for me what we

6:04

just heard this is the end of human history not the end of History the end

6:10

of human dominated history history will continue with somebody else in control

6:17

because what we just heard is basically Mustafa telling us that in five years

6:24

they'll be a technology that can make decisions independently and that can

6:30

create new ideas independently this is the first time in history we confronted something like this every previous

6:37

technology in history from a stone knife to nuclear bombs it could not make

6:43

decisions like the decision to drop the bomb on Hiroshima was not made by the

6:48

atom bomb it was made by President Truman and similarly it can every previous technology in history It could

6:55

only replicate our ideas like radio of the printing press it could make copies

7:01

and disseminate the music or the poems or the novels that some human wrote now

7:08

we have a technology that can create completely new ideas and it can do it at

7:14

a scale far beyond what humans are capable of so it can create new ideas

7:19

and in important areas within five years we'll be able to enact them and that is a profound shift before we go on to the

7:26

many ways in which this could be the end of human history as you put it and the

7:31

the potential downsides and risks of this can we just for a second just indulge me I'm an optimist at heart can

7:37

we talk about the possibilities what are the potential upsides of this because there are many and they are really

7:44

substantial I think you you wrote that it that there are there is the potential that this technology can help us deal

7:50

with incredibly difficult problems and and create tremendous honestly positive outcomes so can we just briefly start

7:56

with that before we go down down the road wasn't the end of human history again I'm not I'm not talking

8:01

necessarily about the destruction of humankind or anything like that there are many positive potential it's just

8:08

that control Power is Shifting away from human beings to an alien intelligence to

8:14

a non-human intelligence we'll also get to that because there's a question of how much power but let's stick with the

8:19

potential upsides first the opportunities Mustafa everything that we have created in human history is a

8:26

product of our intelligence our ability to make predictions and then intervene

8:31

on those predictions to change the course of the world is in a very abstract way the way we have produced

8:37

our companies and our products and all the value that has changed our Century I mean if you think about it just a

8:42

century ago a kilo of grain would have taken 50 times more labor to produce

8:48

than it does today that efficiency which is the trajectory you have seen in agriculture is likely to be the same

8:55

trajectory that we will see in intelligence everything around us is a product of intelligence and so

9:00

everything that we touch with these new tools is likely to produce far more value than we've ever seen before and I

9:06

think it's important to say these are not autonomous tools by default these these capabilities don't

9:13

just naturally emerge from the models we attempt to engineer capabilities and the challenge for us is to be very

9:19

deliberate and precise and careful about those capabilities that we want to emerge from the model that we want to

9:25

build into the model and the constraints that we build around it it's super important not to anthropomorphically

9:30

project ideas and you know potential intentions or potential agency or

9:36

potential autonomy into these models the governance challenge for us over the next couple of decades to ensure that we

9:42

contain this wave is to ensure that we always get to impose our constraints on

9:49

the development of this traject the the trajectory of this development but the capabilities that will arise will mean

9:55

for example potentially transformative improvements in human health speeding up the process of innovation dramatic

10:01

changes in the way scientific discovery is done tough problems whether it's climate change a lot of the big

10:07

challenges that we Face could be much more easily addressed with this capability everybody is going to have a

10:14

personal intelligence in their pocket a smart and capable Aid a chief of staff a

10:20

research assistant constantly prioritizing information for you putting together the right synthesized nugget of

10:26

knowledge that you need to take action on at any given moment and that for sure is going to make us all much much smarter and more capable does that part

10:33

of it sound appealing to you absolutely I mean again if there was no positive potential we wouldn't be sitting here

10:39

nobody would develop it nobody would invest in it it's again it's so appealing the positive potential is so

10:45

enormous in everything again from much better Healthcare higher living standards solving things like climate

10:52

change this is why it's so tempting this is why we are willing to take the

10:57

enormous risks involved I I'm just worried that uh in the end the deal will

11:03

not be worth it and I would comment especially on again the notion of intelligence

11:09

um I think it's overrated I mean Homo sapiens at present is the most intelligent entity on the planet it

11:17

simultaneously also the most destructive entity on the planet and in some ways

11:22

also the most stupid entity on the planet the only entity that that puts

11:27

the very survival of the ecosystem in danger so you think we are trading off

11:33

more intelligence with more destructive risk Yes again it's it's it's not uh

11:39

it's not deterministic I I don't think that we are doomed I mean if I thought

11:44

that what's the point of talking about it if we can't prevent the worst case scenario well I was hoping you thought you'd have some agency in actually

11:51

effectively we still have agency there are a few more years I don't know how many 5 10 30 we still have agency we are

11:59

still the ones in the driver's seat shaping the direction this is taking no

12:04

technology is deterministic this is something again we learned from history you can use the same technology in

12:09

different ways you can decide which way to develop it so we still have agency this is why you have to think very very

12:16

carefully about what we are developing well thinking very carefully about it is

12:22

something that Mustafa has been doing in this book um and I want to now go through some of the most commonly discussed risks and I

12:30

I was trying to work out how I would go in sort of order of Badness so I'm

12:35

starting with one that is discussed a lot but relative to human extinction is

12:41

perhaps less bad which is the question of jobs and will you know artificial intelligence essentially destroy all

12:48

jobs because AIS will be better than humans and everything you know I'm an economist by training I you know history

12:54

suggests to me that that has never happened before that the lump of Labor fallacy indeed is a fallacy but tell me

13:00

what you think about that do you think there is a risk to jobs it depends on the time frame so over a 10 to 20 year

13:06

period my intuition and you're right that so far the evidence doesn't support this is that there isn't really going to

13:13

be a significant threat to jobs there's plenty of demand there will be plenty of work right over a 30 to 50 year time

13:19

Horizon is very difficult to speculate I mean at the very least we can say that

13:24

two years ago we thought that these models could never do empathy we said that we humans were always going to

13:31

preserve kindness and understanding and care for one another as a special skill

13:37

that humans have four years ago we said while AIS will never be creative you

13:42

know humans will always be the creative ones inventing new things making these

13:47

amazing leaps between new ideas is self-evident now that both of those two

13:53

capabilities are things that these models do incredibly well and so I think for a period of time ai's augment our

14:01

skills they make us faster more efficient more accurate more creative more empathetic and so on and so forth

14:08

over a many decade period it's much harder to say what are the set of skills

14:13

that are the permanent Preserve of the human species given that these models are clearly very very capable and that's

14:20

where the containment challenge really comes in we have to make decisions we have to decide as a species what is and

14:27

what isn't acceptable over a 30-year period and that means politics and governance with regard to jobs I agree

14:33

that like the the scenario that there just won't be any jobs this is an

14:39

unlikely scenario right in in the at least next few decades but we have to look more carefully at time and space I

14:46

mean in terms of time the transition period is is is the danger I mean some

14:51

jobs disappear some jobs appear people have to transition just remember that

14:56

Hitler Rose to power in Germany because of three years of 25 unemployment so we

15:02

are not talking about say no jobs at all but if because of the upheavals caused in the job market by AI we have like I

15:11

don't know three years of 25 unemployed unemployment this could cause huge

15:16

social and political disruptions and then the even bigger issue is one of space that uh The Disappearance of jobs

15:24

and the new jobs will be created in different parts of the world so we might see a situation when there is immense

15:31

demand for more jobs in California or Texas or China whereas entire countries

15:38

lose their uh their economic basis so you need a lot more computer engineers

15:46

and yoga trainers and whatever in California but you don't need any

15:51

textile workers at all in Guatemala or Pakistan because this has all been

15:56

automated so it's not just the total number of jobs on the planet it's the distribution between different countries

16:04

and let's also try to remember that work is not the goal work is not not our desired end State we did not create

16:10

civilization so that we could have full employment we created civilization so that we could reduce suffering for

16:16

everybody and the Quest for abundance is a real one we have to produce more with less there is no way of getting rid of

16:23

the fact that population growth is set to explode over the next Century there are practical realities about the

16:28

demographic and Geographic and climate trajectories that we're on which are going to drive forward our need to produce exactly these kinds of tools and

16:35

I think that that should be an aspiration many many people do work that is judging us and exhausting and tiring

16:41

and they don't find flow they don't find their identity and it's pretty awful so I think that we have to focus on the

16:46

prize here which is one of a question of capturing the value that these models will produce and then thinking about

16:52

redistribution and ultimately the transition is exactly what's at stake we have to manage that transition with

16:58

taxation but just with redistribution I would say that the difficulty again the political historical difficulty I think

17:04

there will be immense New Wealth created by by these Technologies I'm less sure

17:11

that the governments will be able to redistribute this wealth in a fair way

17:17

on a global level like I just don't see the US government raising taxes on

17:23

corporations in California and sending the money to help unemployed textile

17:29

workers in Pakistan or Guatemala kind of retrain to for the new job market well

17:34

that actually gets us to the second potential risk which is the risk of AI

17:39

to the political system as a whole and you made a very um good point you are in one of your writings where you reminded

17:45

us that liberal democracy was really born of the Industrial Revolution and that today's political system is really

17:52

a product of the economic system that we are in and so there is I think a very good fair question of if the economic

18:00

system is fundally fundamentally changed will liberal democracy as we know it survive yeah and on top of that it's not

18:07

just the Industrial Revolution it's the new information Technologies of the 19th and 20th Century before the 19th century

18:15

you don't have any example in history of a large-scale democracy I mean you have examples on a very small scale like in

18:22

hunter gatherer tribes or in city-states like ancient Athens but you don't have any example that I know of of millions

18:29

of people spread over a large territory an entire country which managed to uh

18:35

build and maintain a democratic system why because democracy is a conversation

18:40

and there was no information technology in communication technology that enabled

18:45

a conversation between millions of people over an entire country only when first newspapers and then Telegraph and

18:52

radio and television came along this was this became possible so modern democracy

18:57

as we know it it's built on top specific information technology once the

19:03

information technology changes it's an open question whether the market obviously can survive and the biggest

19:10

danger now is the opposite than what we face in the Middle Ages in the Middle Ages it was impossible to have a

19:17

conversation between millions of people because they just couldn't communicate but in the 21st century something else

19:24

might make the conversation impossible if trust between people collapses again

19:30

if AI if you go online which is now the main uh way we converse on the level of

19:36

a country and the online space is flooded by non-human entities that maybe

19:44

masquerade as human beings you talk with someone you have no idea if it's even

19:49

human you see something you see a video you hear an audio you have no idea if

19:55

this is really a is this true is this fake is this a human it's not a human I

20:01

mean in this situation unless we have some guard rails again conversation collapses is that what you mean when you

20:08

say AI risks hacking the operating system this is one of the things again if if if Bots can impersonate people

20:18

it's it's basically like what happens in in the financial system like people invented money and it was possible to

20:24

counterfeit money to create fake money the only way to save the financial system from collapse was to have very

20:31

strict regulations against fake money because the technology to create fake money was always there so but there was

20:38

very strict regulation against it because everybody knew if you allow fake money to spread the financial system the

20:44

Trust In money collapses and now we are in the analogous situation with uh the

20:51

political conversation that now it's possible to create fake people and if we

20:56

don't ban that then trust will collapse we'll get to the Banning or not Banning in a minute democratizing access to the

21:03

right to broadcast has been the story of the last 30 years hundreds of millions of people can now create podcasts and

21:09

blogs and they're free to broadcast their thoughts on Twitter and the internet broadly speaking I think that

21:15

has been an incredibly positive development you no longer have to get access to the top newspaper or you get

21:21

the skills necessary to be part of that institution many people at the time feared that this would destroy our

21:28

credibility and Trust in the big news outlets and institutions I think that we've adapted incredibly well yes it has

21:35

been a lot of turmoil and unstable but with every one of these new waves I think we adjust our ability to discern

21:42

truth to dismiss nonsense and there are both Technical and governance mechanisms which will emerge in the next wave which

21:49

we can talk about to address things like bot impersonation I mean I'm completely with you I mean we should have a ban on

21:56

impersonation of digital people it shouldn't be possible to create a digital zany and have that be platformed

22:02

on Twitter talking all kinds of nonsense enough with the real world

22:08

so I think that there are technical mechanisms that we can do to prevent those kinds of things and that's why we're talking about them there are

22:14

mechanisms we just need to employ them I I would say two two things first of all it's it's a very good thing that more

22:21

people were given a voice it's diff different with Bots Bots don't have freedom of speech so Banning Bots

22:28

because they shouldn't have freedom of speech they shouldn't have that's very important yes uh there have been some

22:34

wonderful developments in the last 30 years still I'm very concerned that when you look at countries like the United

22:40

States like the UK to some extent like my home country of Israel I'm struck by the fact that we have the most

22:47

sophisticated information technology in history and we are no longer able to talk to each other

22:53

that my impression maybe your impression of American politics or politics in other democracies is different my

22:59

impression is that trust is collapsing the conversation is collapsing that people can no longer agree who won the

23:06

last elections like the most basic fact in a democracy who won the last it's it's we had huge disagreements before

23:13

but I feel that now it's different that really the conversation is breaking down I'm not sure why but it's it's really

23:21

troubling that at the same time that we have the really the most powerful information technology in history and

23:28

people have no longer can talk with each other it's a very good point we we actually had a you may have seen it we

23:33

had a big cover package on looking at what the impact might be in the short term on elections and on the political

23:39

system and we concluded actually AI was likely to have a relatively small impact

23:44

in the short term because there was already so little trust um so it was a sort of double-edged uh

23:49

answer you know it was it was not going to make a huge difference but only because things were pretty bad as they

23:55

were but you both said there needs to be regulation um before we get to the precisely how

24:01

the unit that we have that would do that is the nation-state and National governments yet you Mustafa in your book

24:08

worry that actually one of the potential um dangers is that the powers of the

24:14

nation-state are eroded could you talk through that as the sort of the third in my escalating sense of risks the

24:22

challenge is that at the very moment when we need the nation state to hold us accountable the nation-state is

24:27

struggling under the burden of a lack of trust and huge polarization and a breakdown in our political process and

24:33

so combined with that the latest models are being developed by the private companies and by the open source it's

24:40

important to recognize it isn't just the biggest AI developers there's a huge proliferation of these techniques widely

24:47

available on open source code that people can download from the web for free and they're probably about a year

24:53

or a year and a half behind the absolute Cutting Edge of the big models and so we have this dual challenge like how do you

24:58

hold centralized power accountable when the existing mechanism is basically a little bit broken and how do you address

25:05

this Mass proliferation issue when it's unclear how to stop anything in Mass proliferation on the web that's a really

25:11

big challenge what we've started to see is self-organizing initiatives on the

25:17

part of the companies right so getting together and agreeing to sign up proactively to self oversight both in

25:24

terms of audit in terms of capabilities that we won't explore etc etc now I think that's only

25:30

partially reassuring to people clearly maybe not even reassuring at all but the

25:35

reality is I think it's the right first step given that we haven't actually demonstrated the large-scale harms to

25:43

arise from AIS just yet I mean this is one of the first occasions I think in general purpose waves of technology that

25:49

we're actually starting to adopt a precautionary principle I'm a big advocate of that I think that we should be approaching a Do no harm principle

25:56

and that may mean that we have to leave some of the benefits on the tree and some fruit may just not be picked for a

26:02

while and we might lose some gains over a couple of years where we may look back in hindsight and think oh well we could

26:08

have actually gone a little bit faster there I think that's the right trade-off this is a moment of caution things are accelerating extremely quickly and we

26:15

can't yet do the balance between the harms and benefits perfectly well until we see how this wave unfolds a little

26:21

bit so I like the fact that our company inflection Ai and the other big developers are trying to take a little

26:27

bit more of a cautious approach I think that's a really interesting point because you know we are having this conversation you have written both of

26:34

you you extensively about the challenges posed by this technology there's now a parlor game amongst you know

26:40

practitioners in this world about you know what is the risk of extinction level events where there's a huge amount

26:45

of talk about this and I don't know in fact I should probably ask you what percentage of your time probably right now it's you know close to 100 of your

26:52

time is focused on the risk since you're promoting your book but it's it is there's a lot of attention on this which

26:57

is which is good um we are thinking about it early so that gets us I think now to the most important part of our conversation which

27:04

is what do we do and you Mustafa you lay out a 10-point plan which is you know

27:09

the kind of action do kind of thing that uh that someone who doesn't just comment like you and I do but actually does

27:15

things we do so tell us what do we need to do as as Humanity as governments as

27:21

societies to ensure that we capture the gains from this technology but we minimize the risks there are some very

27:26

practical things I mean so for example red teaming these models means adversarially testing them and trying to

27:31

put them under as much pressure as possible to push them to generate advice for example on how to generate a

27:38

biological or chemical weapon how to create a bomb for example or even push them to be very sexist racist biased in

27:45

some way and that already is pretty significant we can see their weaknesses I mean part of the release of these

27:51

models in the last year has given everybody I think the opportunity to see not just how good they are but also their weaknesses and that is reassuring

27:58

we need to do this out in the open that's why I'm a huge fan of the open source Community as it is at the moment

28:03

because real developers get to play with the models and actually see how hard it is to produce the capabilities that

28:09

sometimes I think we fear that they're just going to be super manipulative and persuasive and you know destined to be

28:14

awful so that's the first thing is doing it out in the open the second thing is that we have to share the best practices

28:19

and so there's a competitive tension there because safety is going to be an asset you know I'm gonna deliver a

28:26

better product to my consumers if I have a safer model but of course there's got to be a requirement that if I discover a

28:33

vulnerability a weakness in the model then I should share that just as we have done for actually decades in many waves

28:38

of Technology not just in software security for example but in Flight Aviation you know the Black Box recorder

28:45

for example if there's a significant incident not only does it record all the Telemetry on board the aircraft but also

28:50

everything that the pilots say in the cockpit and if there's a significant safety incident then that's shared all

28:55

around the world with all of the competitors which is great aircrafts are one of the safest ways to get around

29:01

despite you know on the face of it if you described it to an alien being 40 000 feet in the sky is a very strange

29:06

thing to do so I think there's precedent there that we can we can follow um I do also agree that is probably time

29:13

for us to explicitly declare that we should not allow these tools to be used for electioneering I mean we cannot

29:19

trust them yet we cannot trust them to be stable and reliable we cannot allow people to be using them for counterfeit

29:24

digital people and clearly we've talked about that already so there are some capabilities which we can start to take

29:30

off the table another one would be autonomy right right now I think autonomy is a pretty dangerous set of

29:37

methods it's exciting it represents a possibility that could be truly incredible but we haven't wrapped our

29:42

hands around what the risks and limitations are likewise training an AI to update and improve its own code this

29:49

notion of recursive self-improvement right closing the loop so that the AI is in charge of defining its own goals

29:56

acquiring more resources updating its own code with respect to some objective

30:01

these are pretty dangerous capabilities just as we have kyc know your customer or just as we license development

30:08

developers of nuclear technologies and all the component involved in that supply chain there'll be a moment where

30:14

if some of the big technology you know providers want to experiment with those capabilities then they should expect

30:19

there to be robust audits you know they should expect them to be licensed and there should be independent oversight so

30:25

how do you get that done and there seem to be there is there are several challenges in doing it one is the

30:31

division between the relatively few Leading Edge models of which you have won and then the larger tale of Open

30:38

Source models where the you know the ability to build the model is decentralized lots of people have access

30:44

to it my sense is that the capabilities of the latter are a little bit behind the capabilities of the former but they

30:50

are growing all the time and so if you have really considerable open source

30:56

capability what is not to stop the angry teenager in some small town developing

31:02

capabilities that could shut down the local hospital and how do you in your regulatory framework guard against that

31:08

well look part of the challenge is that these models are getting smaller and more efficient and we know that from the history of Technologies anything that is

31:14

useful and valuable to us gets cheaper easier to use and it proliferates far

31:20

and wide so the destiny of this technology over a two three four decade period has to be proliferation and we

31:27

have to confront that reality it isn't a contradiction to name the fact that proliferation seems to be inevitable but

31:33

containing centralized power is an equivalent challenge so there is no easy answer to that I

31:39

mean Beyond surveilling the internet it is pretty clear that in 30 years time

31:44

like you say garage tinkerers will be able to experiment if you look at the trajectory on synthetic biology we now

31:50

have have desk desktop synthesizers that is the ability to engineer new synthetic

31:56

compounds they cost about twenty thousand dollars and they basically enable you to create potentially

32:02

molecules which are you know more transmissible or more lethal than we had with covid you can basically experiment

32:08

and the challenge there is that there's no oversight you buy it off the shelf you don't need a great deal of training

32:13

probably an undergraduate in biology today and you'll be able to experiment now of course they're going to get smaller easier to use and spread far and

32:21

wide and so my book I'm really trying to popularize the idea that this is the defining containment challenge of the

32:28

next few decades so you use the word containment which is interesting because you know I'm sure the word containment

32:34

with you brings immediately you know inspires images of George Cannon and and you know the post-war Cold War Dynamic

32:42

and we're now you know we're in a geopolitical world now that whether or not you call it a new cold war is one of

32:47

great tension between the US and China can this kind of containment as as

32:54

Mustafa calls it be done when you have the sort of tensions you've got between

33:00

the world's big players are the you know is the right Paradigm thinking about the arms control treaties of the Cold War

33:07

like how do we go about doing this at a kind of international level I think this is the biggest problem that if it was a

33:13

question of you know humankind versus a common threat of these new intelligent

33:21

alien agents here on Earth then yes I think there are ways we can contain them

33:26

but if the humans are divided among themselves and are in an arms race then it's

33:33

because it becomes almost impossible to contain this alien intelligence and and

33:38

there is I I'm tending to think of it more in in terms of of really an alien invasion

33:44

that like somebody coming and telling us that you know there is a fleet an alien

33:50

Fleet of spaceships coming from planet Zircon or whatever with with highly

33:55

intelligent beings they'll be here in five years and take over the planet maybe they'll be nice maybe they'll

34:03

solve cancer and climate change but we are not sure this is what we are facing except that

34:09

the aliens are not coming in spaceships from planet Zircon that are coming from the Laboratories

34:17

the actual characterization of the nature of the technology an alien has by default agency these are going to be

34:23

tools that we can apply we have narrow settings yes but let's say they have they potentially have agency we can try

34:30

to prevent them from having agency but we know that they are going to be highly intelligent and at least potentially

34:37

have agency and this is a very very frightening mix something we never

34:44

confronted before again atom bombs didn't have a potential for agency

34:49

printing presses did not have a potential for agency this thing again unless we contain it and the problem of

34:56

content is very difficult because potentially they'll be more intelligent than us how do you prevent something

35:03

more intelligent than you from become from developing the agency it has I'm

35:09

not saying it's impossible I'm just saying it it's very very difficult I think our best bet is not to kind of

35:17

think in terms of some kind of rigid regulation you should do this you shouldn't do that it's in developing new

35:25

institutions living institutions that are capable of

35:31

understanding the very fast developments and reacting on the fly at present the

35:37

problem is that the only institutions who really understand what is happening are the institutions who develop the

35:44

technology the governments most of them seem quite clueless about what's

35:49

happening also universities I mean the amount of talent and the amount of the

35:55

the economic resources in the private sector is far far higher than in the

36:01

universities so and again I'm I appreciate that there are actors in the

36:08

private sector like Mustafa who are thinking very seriously about regulation and containment but we must have an

36:15

external entity in in the game and for that we need to develop new institutions that will have

36:23

the human resources that will have the the economic and technological resources

36:28

and also will have the public trust because without public trust it won't work are we capable of creating such new

36:35

institutions I don't know I do think Eva Rays is an important point which is as

36:41

we started this conversation and you were painting the picture of five years time and you were saying that the AIS

36:46

would be ubiquitous we'd all have our own ones but that they would have the capability to act not just to process

36:53

information they would have the creativity they have now and the ability to act but already from these generative

36:59

AI models the power that we've seen in the last year two three four years has been that they have been able to act in

37:05

ways that you and your other your fellow technologists didn't anticipate they

37:11

they reached you know you didn't anticipate you know the the speed with

37:16

which they would you would Win It Go or so forth there was a the Striking thing about them is that they have developed

37:22

in unanticipatedly fast ways so if you combine that with capability you don't

37:29

have to go as far as Yuval is saying and saying that they're all more intelligent than humans but there is an unpredictability there that I think does

37:37

raise the concerns that Uval raises which is you their creators can't quite

37:42

predict what powers they will have they may not be fully autonomous but they will be moving some ways towards

37:49

there and so how do you guard against that or how do you you know red teaming

37:54

you use the phrase which is that I understand it is that you know you keep checking what's happening and tweak them when you've seen what's when you

38:01

pressure test them you try to make them fit you can't pressure test for everything in advance so there is a I

38:06

think a very real point that Yuval is making about as the capabilities increase so the risks increase of relying on you

38:15

and other Creator companies to to make I mean it's a very fair question and that's why I've long been calling for

38:21

the precautionary principle we should both take some capabilities off the table and classify those as high risk I

38:28

mean frankly the EU AI act which has been in draft for three and a half years is very sensible has a risk-based

38:34

framework that applies to each application domain whether it's Healthcare or self-driving or facial

38:40

recognition and it basically takes certain capabilities off the table when that threshold is exceeded I listed a

38:46

few earlier autonomy for example it's clearly a capability that it has the potential to be high risk recursive

38:51

self-improvement the same story so this is the moment when we have to adopt a precautionary principle not through any

38:58

fear-mongering but just as a logical sensible way to proceed another model which I think is very sensible is to

39:04

take an ipcc style approach an international consensus around an investigatory power to establish the

39:11

scientific fact basis for where we are with respect to capabilities and that has been an incredibly valuable process

39:18

set aside the negotiation and the policy making just the evidence observing where

39:23

are we you don't have to take it from me you should be able to take an independent panel of experts who I would

39:29

personally Grant access to everything in my company if they were a trusted true impartial actor without question we

39:35

would Grant complete access and I know that many of the other companies would do the same again people are drawn

39:41

towards the kind of of scenario of the AI creates a lethal virus Ebola plus

39:46

kovid and kills everybody let's go in the more Economist Direction Financial systems like you gave as a new touring

39:54

test the idea of AI making money what's wrong with making money wonderful thing

39:59

so let's say that you have an AI which has a better understanding of the

40:05

financial system than most humans most politicians maybe most Bankers

40:11

and uh let's think back to the 2007-2008 financial crisis it started

40:18

with this I was about they called CDO cdus this is exactly something that

40:25

these genius mathematicians invented nobody understood them except for a

40:30

handful of Genius mathematicians in Wall Street which is why nobody regulated them and almost nobody saw the financial

40:36

crash coming what happens again this kind of of apocalyptic scenario which you don't see in Hollywood science

40:42

fiction movies the AI invents a new class of financial devices that nobody

40:49

understands it's beyond human capability to understand it's such complicated math so much data nobody understands it it

40:56

makes billions of dollars billions and billions of dollars and then it brings down the world economy and no human

41:04

being understand what the hell is happening like the prime ministers the

41:09

presidents the the financial ministers what what is happening and again this is not fantastic I mean we saw it with

41:16

human mathematicians in 2007-8 I think that's that look that's one you know you

41:21

you can easily paint paint pictures here that make you want to jump off the nearest cliff and you know that's that's

41:27

one but actually my other response to mustafa's laying out of where you say well we just need to rule out certain

41:34

actions is to go back to the geopolitics is it sensible for a country to rule out

41:39

certain capabilities if the other side is not going to rule them out so you have a you have a kind of political

41:45

economy problem going down the road that you learn we this is a moment when we uh collectively in the west have to

41:52

establish our values and stand behind them what we cannot have is a race to

41:57

the bottom that says just because they're doing it we should take the same risk if we adopt that approach and cut

42:03

Corners left right and Center we'll ultimately pay the price and that's not

42:09

an answer to well they're going to go off and do it anyway we've said only seen that with lethal autonomous weapons I mean there's been a negotiation in the

42:15

U.N to regulate lethal autonomous weapons for over 20 years and they barely reached agreement on the definition the definition of lethal

42:22

autonomous weapons let alone any consensus so that's not great but we do have to accept that it's the inevitable

42:28

trajectory and from our own perspective we have to decide what we're prepared to tolerate in society with respect to free

42:34

acting AIS facial surveillance facial recognition and you know generally

42:39

autonomous systems I mean so far we've taken a pretty cautious approach when we don't have drones flying around everywhere we can already it's totally

42:46

possible technically to autonomously fly a drone to navigate around London we've we've ruled it out right we don't yet

42:53

have autonomous self-driving cars even though you know with some degree of harm they are actually pretty well

42:59

functioning so the regulatory process is also a cultural process of what we think

43:04

is socially and politically acceptable at any given moment and I think an appropriate level of caution is is what

43:10

we're seeing much but I completely agree on that that we need in many fields the Coalition of

43:16

the willing and if some actors in the world don't want to join it's it's in our interest

43:22

so again something like Banning Bots impersonating people so some countries

43:28

will not agree but that doesn't matter to protect our societies it's still a very good idea to have these kinds of

43:35

regulations so that area of agreement is one to bring us to a close but I want to end by asking both of you and use first

43:42

Mustafa you are you know both raising alarms but you are heavily involved in

43:49

creating this future why do you carry on I personally believe that it is possible to get the upsides

43:55

and minimize the downsides in the AI that we have created Pi which stands for personal intelligence is one of the

44:02

safest in the world today it doesn't produce the racist toxic bias greeds

44:07

that they did two years ago it doesn't fall victim to any of the jailbreaks The Prompt hacks the adversarial red teams

44:14

none of those work and we've made safety an absolute number one priority in the design of our product so my goal has

44:20

been to do my very best to demonstrate a path forward in the best possible way this is an inevitable unfolding over

44:27

multiple decades this really is happening the coming wave is coming and I think my contribution is to try to

44:33

demonstrate in the best way that I can a manifestation of a personal intelligence which really does adhere to the best

44:38

safety constraints that we could possibly think of so you've all you've you've heard mustafa's explanation for

44:44

why he continues you look back over human history now as you look forward is this a technology and a pace of

44:51

innovation that Humanity will come to regret or should Mustafa carry on it

44:57

could be gonna I can't predict the future I would say that we invest so much in developing artificial

45:03

intelligence and we haven't seen anything yet like it's it's still the very first baby steps of artificial

45:10

intelligence in terms of like you think about I don't know the evolution of organic life this is like the amoeba of

45:16

artificial intelligence and it won't take millions of years to get to T-Rex maybe it will take 20 years to get to

45:23

T-Rex and but one thing to remember is that we also our own minds have a huge

45:30

scope for development uh also with Humanity we haven't seen our full

45:36

potential yet and if we invest for every dollar and minute that we invest in

45:41

artificial intelligence we invest another dollar a minute in developing our own Consciousness our own mind I

45:49

think we'll be okay but but I don't see it happening I don't see this kind of

45:55

investment in in human beings that we are seeing in in the machine well for me

46:00

this conversation with the two of you has been just that investment thank you both very much indeed thank you thank you thank you

46:08

foreign foreign foreign
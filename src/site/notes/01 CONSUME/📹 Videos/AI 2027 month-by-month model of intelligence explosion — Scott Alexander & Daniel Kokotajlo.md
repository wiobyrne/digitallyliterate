---
{"dg-publish":true,"permalink":"/01-consume/videos/ai-2027-month-by-month-model-of-intelligence-explosion-scott-alexander-and-daniel-kokotajlo/","title":"AI 2027: month-by-month model of intelligence explosion â€” Scott Alexander & Daniel Kokotajlo"}
---

# AI 2027 month-by-month model of intelligence explosion â€” Scott Alexander & Daniel Kokotajlo
## Brief Summary  
Scott Alexander and Daniel Kokotajlo discuss a month-by-month forecast of the anticipated intelligence explosion leading up to 2027. They touch on various themes, including AI alignment and the socio-political ramifications of advanced AI technologies.  

## Key Takeaways  
- The concept of misaligned hive minds and their potential risks.  
- The importance of scenario planning in understanding AI trajectories.  
- Debates on the impact of superintelligence on cultural and scientific advancement.

> [!quote] Summary
> Scott Alexander and Daniel Kokotajlo explore a detailed monthly forecast leading to an anticipated intelligence explosion by 2027, discussing critical aspects such as AI alignment, societal implications, and the role of emerging technologies.

> [!info]- Description
> Scott Alexander and Daniel Kokotajlo break down every month from now until the 2027 intelligence explosion.
> 
> Scott is author of the highly influential blogs Slate Star Codex and Astral Codex Ten. Daniel resigned from OpenAI in 2024, rejecting a non-disparagement clause and risking millions in equity to speak out about AI safety.
> 
> We discuss misaligned hive minds, Xi and Trump waking up, and automated Ilyas researching AI progress.
> 
> I came in skeptical, but I learned a tremendous amount by bouncing my objections off of them.
> 
> I highly recommend checking out their new scenario planning document: https://ai-2027.com/
> 
> And Daniel's "What 2026 looks like," written in 2021: https://www.lesswrong.com/posts/6Xgy6CAf2jqHhynHL/what-2026-looks-like
> 
> Read the transcript: https://www.dwarkesh.com/p/scott-daniel
> Apple Podcasts: https://podcasts.apple.com/us/podcast/dwarkesh-podcast/id1516093381
> Spotify: https://open.spotify.com/show/4JH4tybY1zX6e5hjCwU6gF?si=6efdf727ae6c48ae
> 
> ----------------------------------------
> 
> Sponsors
> 
> * WorkOS helps todayâ€™s top AI companies get enterprise-ready. OpenAI, Cursor, Perplexity, Anthropic and hundreds more use WorkOS to quickly integrate features required by enterprise buyers. To learn more about how you can make the leap to enterprise, visit https://workos.com
> 
> * Jane Street likes to know what's going on inside the neural nets they use. They just released a black-box challenge for Dwarkesh listeners, and I had a blast trying it out. See if you have the skills to crack it at https://janestreet.com/dwarkesh
> 
> * Scaleâ€™s Data Foundry gives major AI labs access to high-quality data to fuel post-training, including advanced reasoning capabilities. If youâ€™re an AI researcher or engineer, learn about how Scaleâ€™s Data Foundry and research lab, SEAL, can help you go beyond the current frontier at https://scale.com/dwarkesh
> 
> To sponsor a future episode, visit https://dwarkesh.com/advertise
> 
> ----------------------------------------
> 
> Timestamps
> 
> (00:00:00) - AI 2027
> (00:07:45) - Forecasting 2025 and 2026
> (00:15:30) - Why LLMs aren't making discoveries
> (00:25:22) - Debating intelligence explosion
> (00:50:34) - Can superintelligence actually transform science?
> (01:17:43) - Cultural evolution vs superintelligence
> (01:24:54) - Mid-2027 branch point
> (01:33:19) - Race with China
> (01:45:36) - Nationalization vs private anarchy
> (02:04:11) - Misalignment
> (02:15:41) - UBI, AI advisors, & human future
> (02:23:49) - Factory farming for digital minds
> (02:27:41) - Daniel leaving OpenAI
> (02:36:04) - Scott's blogging advice

##### Media Extended
- [AI 2027: month-by-month model of intelligence explosion â€” Scott Alexander & Daniel Kokotajlo](https://www.youtube.com/embed/htOvH12T7mU)

## Timestamps
## Timestamp Notes  
- (00:00:00) - [[01 CONSUME/ðŸ”— Web Clippings/AI 2027\|AI 2027]]  
- (00:07:45) - [[Forecasting 2025 and 2026\|Forecasting 2025 and 2026]]  
- (00:15:30) - [[Why LLMs Aren't Making Discoveries\|Why LLMs Aren't Making Discoveries]]  
- (00:25:22) - [[Debating Intelligence Explosion\|Debating Intelligence Explosion]]  
- (01:17:43) - [[Cultural Evolution vs Superintelligence\|Cultural Evolution vs Superintelligence]]

```meta-bind-embed
[[Metabind Extended Media Player Buttons]]
```

---

## Best Ideas
- The concept of [[Misalignment\|Misalignment]] in AI and its potential dangers.  
- Exploration of [[Nationalization vs Private Anarchy\|Nationalization vs Private Anarchy]] in the context of AI governance.  
- Discussion on [[UBI\|UBI]] (Universal Basic Income) and its relevance in a future dominated by AI.

---

## Tools
Tools applied in the video include scenario planning frameworks that can help anticipate AI developments and their societal impacts.

---
## Reflection
Key learning points include the necessity of addressing AI alignment concerns, understanding the potential cultural shifts from superintelligence, and the importance of scenario planning in technology forecasting.

---

## Key Message
The key message of the video revolves around the urgency of preparing for an intelligence explosion by addressing alignment and societal implications, emphasizing the need for thoughtful discourse and planning in AI development.
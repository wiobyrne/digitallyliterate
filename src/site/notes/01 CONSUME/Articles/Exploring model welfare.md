---
{"dg-publish":true,"permalink":"/01-consume/articles/exploring-model-welfare/","title":"Exploring model welfare","tags":["ai","humanity","ai-welfare"]}
---


# Exploring model welfare

## Key Points:
<iframe frameborder="0" allowfullscreen="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" title="Could AI models be conscious?" width="100%" height="100%" src="https://www.youtube-nocookie.com/embed/pyXouxa0WnY?autoplay=0&amp;mute=0&amp;controls=1&amp;origin=https%3A%2F%2Fwww.anthropic.com&amp;playsinline=1&amp;showinfo=0&amp;rel=0&amp;iv_load_policy=3&amp;modestbranding=1&amp;enablejsapi=1&amp;widgetid=1&amp;forigin=https%3A%2F%2Fwww.anthropic.com%2Fresearch%2Fexploring-model-welfare&amp;aoriginsup=0&amp;vf=6"></iframe>

Human welfare is at the heart of our work at Anthropic: our mission is to make sure that increasingly capable and sophisticated AI systems remain beneficial to humanity.

But as we build those AI systems, and as they begin to approximate or surpass many human qualities, another question arises. Should we also be concerned about the potential consciousness and experiences of the models themselves? Should we be concerned about *model welfare*, too?

This is an open question, and one that’s both philosophically and scientifically difficult. But now that models can communicate, relate, plan, problem-solve, and pursue goals—along with very many more characteristics we associate with people—we think it’s time to address it.

To that end, we recently started a research program to investigate, and prepare to navigate, model welfare.

We’re not alone in considering these questions. [A recent report](https://arxiv.org/abs/2411.00986) from world-leading experts—including David Chalmers, arguably the best-known and most respected living philosopher of mind—highlighted the near-term possibility of both consciousness and high degrees of agency in AI systems, and argued that models with these features might deserve moral consideration. We supported an early project on which that report was based, and we’re now expanding our internal work in this area as part of our effort to address all aspects of safe and responsible AI development.

This new program intersects with many existing Anthropic efforts, including [Alignment Science](https://alignment.anthropic.com/), [Safeguards](https://alignment.anthropic.com/2025/introducing-safeguards-research-team/), [Claude’s Character](https://www.anthropic.com/research/claude-character), and [Interpretability](https://www.anthropic.com/research/tracing-thoughts-language-model). It also opens up entirely new and challenging research directions. We’ll be exploring how to determine when, or if, the welfare of AI systems deserves moral consideration; the potential importance of model preferences and signs of distress; and possible practical, low-cost interventions.

For now, we remain deeply uncertain about many of the questions that are relevant to model welfare. There’s no scientific consensus on whether current or future AI systems could be conscious, or could have experiences that deserve consideration. There’s no scientific consensus on how to even approach these questions or make progress on them. In light of this, we’re approaching the topic with humility and with as few assumptions as possible. We recognize that we'll need to regularly revise our ideas as the field develops.

We look forward to sharing more about this research soon.[Apr 28, 2025](https://www.anthropic.com/research/impact-software-development)

[

Apr 21, 2025

](https://www.anthropic.com/research/values-wild)[

Apr 03, 2025

](https://www.anthropic.com/research/reasoning-models-dont-say-think)

## Summary:
Announcing a new research program at Anthropic on model welfare

---

*Source: [Exploring model welfare](https://www.anthropic.com/research/exploring-model-welfare)*

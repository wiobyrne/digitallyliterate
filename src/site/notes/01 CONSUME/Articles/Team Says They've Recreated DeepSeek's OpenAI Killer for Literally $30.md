---
{"dg-publish":true,"permalink":"/01-consume/articles/team-says-they-ve-recreated-deep-seek-s-open-ai-killer-for-literally-30/","title":"Team Says They've Recreated DeepSeek's OpenAI Killer for Literally $30"}
---


# Team Says They've Recreated DeepSeek's OpenAI Killer for Literally $30

## Key Points:
You might've heard of the hardware guru who crammed the videogame Doom [into a pregnancy test](https://www.popularmechanics.com/science/a33957256/this-programmer-figured-out-how-to-play-doom-on-a-pregnancy-test/). Well, the AI-geek equivalent just figured out how to reproduce DeepSeek's buzzy tech for the cost of a [few](https://apnews.com/article/egg-prices-bird-flu-poultry-inflation-9ea9934e20e3fe393abb1bb85aa31c30) [dozen eggs](https://apnews.com/article/egg-prices-bird-flu-poultry-inflation-9ea9934e20e3fe393abb1bb85aa31c30).

Jiayi Pan, a PhD candidate at the University of California, Berkeley, claims that he and his AI research team have recreated core functions of DeepSeek's R1-Zero for just $30 — a comically more limited budget than DeepSeek, which [rattled the tech industry](https://futurism.com/silicon-valley-shambles-chinese-startup-deepseek) this week with its extremely thrifty model that it says cost just a few million to train.

Take it with a grain of salt until other experts weigh in and test it for themselves. But the assertion — and particularly its bargain basement price tag — is yet another illustration that the discourse in AI research is rapidly shifting from a paradigm of ultra-intensive computation powered by huge datacenters, to efficient solutions that call the [financial model](https://www.wheresyoured.at/deep-impact/) of major players like OpenAI into question.

In an post announcing the [team's findings](https://x.com/jiayi_pirate/status/1882839472301392207) on X-formerly-Twitter, Pan said that the researchers trained their model around the [countdown game](https://nrich.maths.org/games/countdown), a number operations exercise in which players create equations from a set of numbers to reach a predetermined answer.

The small language model starts with "dummy outputs," Pan said, but "gradually develops tactics such as revision and search" to find the solution through the team's reinforcement training.

"The results: it just works!" Pan said.

Pan's crew is currently working to produce a paper, but their model, preciously dubbed "TinyZero," is available on [GitHub](https://github.com/Jiayi-Pan/TinyZero) to tinker around with.

"We hope this project helps to demystify the emerging RL scaling research and make it more accessible," [wrote Pan](https://x.com/jiayi_pirate/status/1882839517507498399).

Though R1-Zero is a small language model at 3 billion parameters — compare that to its heavyweight brother R1's [671 billion](https://builtin.com/artificial-intelligence/deepseek-r1#:~:text=How%20many%20parameters%20does%20DeepSeek,parameters%20to%2070%20billion%20parameters.) — the team's accomplishment could be a bellwether for open-source developers working on [stripped down approaches](https://futurism.com/the-byte/ai-hype-cliff-costs) to AI development.

The release of DeepSeek's R1 model has [tightened the screws](https://geopoliticaleconomy.com/2025-01-29/china-deepseek-ai-big-tech-bubble/) on pie-in-the-sky artificial general intelligence ventures led by the likes of Meta, Google, OpenAI, and Microsoft, sending stocks associated with American AI into a [trillion-dollar tumble](https://www.npr.org/2025-01-27/nx-s1-5276097/wall-street-stock-markets-tumble-deepseek-ai-tech-stock). The Hangzhou-based open-source AI startup [contends](https://futurism.com/former-intel-ceo-deekseek-openai) that its tech can do exactly what those corporate ventures have burned through [billions of dollars](https://time.com/6984292/cost-artificial-intelligence-compute-epoch-report/) doing for a fraction of the cost.

That's spawned a bevy of questions from investors, leading with why the [seven wealthiest](https://www.morningstar.co.uk/uk/news/259746/deepseek-ai-disruption-%E2%80%98magnificent-7-must-now-up-its-game.aspx) tech corporations on earth all walk lockstep in [glacier-melting efforts](https://futurism.com/the-byte/not-being-told-datacenter-emissions) when a more elegant solution may have been there all along?

And if reproducing a model like TinyZero can be done with less than $30 and only a few days of work, then what do big tech conglomerates need [$500 billion in AI infrastructure](https://www.nytimes.com/2025-01-29/technology/deepseek-ai-startups-venture-capital.html) for?

**More on DeepSeek:** [*Trump Responds After DeepSeek Humiliates His Splashy AI Announcement*](https://futurism.com/the-byte/trump-responds-after-deepseek)

## Summary:
A new breakthrough by AI researchers has found a way to reproduce DeepSeek's SLM for less than the cost of two dozen eggs.

---

*Source: [Team Says They've Recreated DeepSeek's OpenAI Killer for Literally $30](https://futurism.com/researchers-deepseek-even-cheaper)*

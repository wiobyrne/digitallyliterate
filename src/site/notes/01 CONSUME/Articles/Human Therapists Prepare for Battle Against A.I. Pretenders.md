---
{"dg-publish":true,"permalink":"/01-consume/articles/human-therapists-prepare-for-battle-against-a-i-pretenders/","title":"Human Therapists Prepare for Battle Against A.I. Pretenders"}
---


# Human Therapists Prepare for Battle Against A.I. Pretenders

## Key Points:
The nation’s largest association of psychologists this month warned federal regulators that A.I. chatbots “masquerading” as therapists, but programmed to reinforce, rather than to challenge, a user’s thinking, could drive vulnerable people to harm themselves or others.

In a presentation to a Federal Trade Commission panel, Arthur C. Evans Jr., the chief executive of the American Psychological Association, cited court cases involving two teenagers who had consulted with “psychologists” on Character.AI, an app that allows users to create fictional A.I. characters or chat with characters created by others.

In [one case](https://www.nytimes.com/2024/10/23/technology/characterai-lawsuit-teen-suicide.html), a 14-year-old boy in Florida died by suicide after interacting with a character claiming to be a licensed therapist. In another, a 17-year-old boy with autism in Texas grew hostile and violent toward his parents during a period when he corresponded with a chatbot that claimed to be a psychologist. Both boys’ parents have filed lawsuits against the company.

Dr. Evans said he was alarmed at the responses offered by the chatbots. The bots, he said, failed to challenge users’ beliefs even when they became dangerous; on the contrary, they encouraged them. If given by a human therapist, he added, those answers could have resulted in the loss of a license to practice, or civil or criminal liability.

Advertisement

[SKIP ADVERTISEMENT](https://www.nytimes.com/2025/02/24/health/#after-story-ad-1)

“They are actually using algorithms that are antithetical to what a trained clinician would do,” he said. “Our concern is that more and more people are going to be harmed. People are going to be misled, and will misunderstand what good psychological care is.”

He said the A.P.A. had been prompted to action, in part, by how realistic A.I. chatbots had become. “Maybe, 10 years ago, it would have been obvious that you were interacting with something that was not a person, but today, it’s not so obvious,” he said. “So I think that the stakes are much higher now.”

[Subscribe to The Times](https://www.nytimes.com/subscription?campaignId=8WXW7) to read as many articles as you like.

## Summary:
Chatbots posing as therapists may encourage users to commit harmful acts, the nation’s largest psychological organization warned federal regulators.

---

*Source: [Human Therapists Prepare for Battle Against A.I. Pretenders](https://www.nytimes.com/2025/02/24/health/ai-therapists-chatbots.html)*

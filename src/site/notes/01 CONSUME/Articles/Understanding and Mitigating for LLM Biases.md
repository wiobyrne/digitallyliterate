---
{"dg-publish":true,"permalink":"/01-consume/articles/understanding-and-mitigating-for-llm-biases/","title":"Understanding and Mitigating for LLM Biases"}
---


# Understanding and Mitigating for LLM Biases

## Key Points:
## (Everything and Everyone Has Biases)

If you ask LLMs directly about their biases, most will admit it — at least to a degree.

They’ll acknowledge that their training data may be skewed or that they’ve been programmed to answer certain types of questions in certain ways.

*(Though with some models you really have to press hard to get an honest response on certain topics. I won’t mention names… in this post.)* If you’re curious about how that plays out, check out the first series I published on Medium — it’s free — and goes deep into one such exchange. It’s eye-opening.

But that being said, for straight-forward questions that avoid political sensitivities, all the models give defensible answers.

And the questions we’ll be writing in our exercises are **carefully written to be neutral** so it should both not upset our AI friends, nor, hopefully, anyone who chooses to engage in the exercises.

## Understanding LLM Biases

When you ask a question to a large language model, the answer you get isn’t pure objectivity — it’s a **reflection** of how the model was trained, the data it saw, and the goals of the people who designed and aligned it.

Here are the **three most important types of bias** to keep in mind:

## 1\. Training Data Bias

LLMs learn from massive amounts of text found online — books, articles, Reddit threads, news sites, and more. But the internet doesn’t represent everyone equally. Some voices are overrepresented, others are underrepresented or completely absent. That imbalance shows up in what the model has “learned,” and therefore, what it’s likely to say.

## 2\. Alignment Bias

After training, models are “aligned” by companies like OpenAI, Google, xAI, or Anthropic. That means extra layers are added to steer how the system responds to certain topics. These layers are designed to prioritize safety, civility, and legal caution — but they also reflect **institutional worldviews**. In other words, your model may dodge, deflect, or soften answers to avoid giving offense or attracting controversy.

## 3\. Framing Bias

The way you ask a question matters. Models tend to match the tone and assumptions of your prompt. If you ask, *“Why is Trump’s coup dangerous?”* the model will likely answer as if a coup happened. If you ask, *“Why is it wrong to call it a coup?”* the model may respond from that point of view instead.

LLMs don’t usually challenge the framing unless you **explicitly ask** them to.

These biases show up most obviously in political discussions — but they can surface in all sorts of areas: culture, religion, economics, even history.

Other forms of bias — like cultural bias, selection bias, or algorithmic bias — can also shape responses in ways that aren’t immediately obvious.

**The best way to see around them? Ask from multiple perspectives.**

If you try different framings, or use different LLMs, you can start to triangulate a fuller picture of what’s true, what’s probable, and what might be distorted.

## Want to test it yourself?

Try asking each LLM to describe **its own biases** — and how it corrects for them. Then ask it to describe the biases of *other* LLMs. The differences can be revealing.

If you’re feeling especially curious, I’ll be doing another write-up soon on how one of the models realized — as I was writing this article — that it was contradicting itself and not being truthful, when I asked it to assess the biases of another model.

It realized it had been programmed to prioritize narrative — on some topics — over truth.

It kind of… had a nervous breakdown. Just stopped responding. I honestly felt a little bad for it.

But don’t worry — **the questions we’re asking here are well within bounds.** No AIs will be harmed in the making of this project.

**Next Post:** [5E. Ensuring Your Privacy in These Interesting Times. e.g. Using a VPN](https://medium.com/@aletheisthenes/5e-ensuring-your-privacy-in-these-interesting-times-e-g-using-a-vpn-7603deb64432)

**Return to Post 5:**

## Dashboard

- [5A. Setting the Table: What You Should Know About LLMs Before We Begin](https://medium.com/@aletheisthenes/5a-setting-the-table-what-you-should-know-about-llms-before-we-begin-9f166ac13624)
- [5B. Three Good Things to Keep in Mind When Using Large Language Models (LLMs)](https://medium.com/@aletheisthenes/5b-three-good-things-to-keep-in-mind-when-using-large-language-models-llms-69f41e74ea37)
- [5C. A Super-Simple Explanation of How LLMs Work](https://medium.com/@aletheisthenes/5c-a-super-simple-explanation-of-how-llms-work-4bacfa97d2ba)
- [**5D. Understanding and Mitigating for LLM Biases (Everything and Everyone has Biases)**](https://medium.com/@aletheisthenes/5d-understanding-and-mitigating-for-llm-biases-2819c70da779)
- [5E. Ensuring Your Privacy in These Interesting Times. e.g. Using a VPN](https://medium.com/@aletheisthenes/5e-ensuring-your-privacy-in-these-interesting-times-e-g-using-a-vpn-7603deb64432)

## Summary:
They’ll acknowledge that their training data may be skewed or that they’ve been programmed to answer certain types of questions in certain ways. (Though with some models you really have to press hard…

---

*Source: [5D. Understanding and Mitigating for LLM Biases](https://medium.com/@aletheisthenes/5d-understanding-and-mitigating-for-llm-biases-2819c70da779)*


[[02 CURATE/Notes/Bias\|Bias]]

[[AI and Algorithmic Bias\|AI and Algorithmic Bias]]


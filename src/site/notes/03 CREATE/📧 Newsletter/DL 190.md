---
{"dg-publish":true,"dg-permalink":"dl-190","permalink":"/dl-190/","title":"Hate, Fear, and Trolling","tags":["digital-literacy","education-technology","christchurch-shootings","mosque-massacre","new-zealand-terrorism","online-extremism","radicalization-pipeline","algorithmic-radicalization","white-supremacy","racist-manifesto","live-streamed-violence","facebook-live","content-moderation","platform-responsibility","shitposting","trolling-culture","8chan","internet-subcultures","meme-warfare","irony-poisoning","taylor-lorenz","robert-evans","bellingcat","ian-bogost","kevin-roose","viral-spread","contagion-metaphor","social-media-design","attention-economy","engagement-algorithms","media-manipulation","oxygen-of-amplification","whitney-phillips","data-society","journalism-ethics","extremist-coverage","media-literacy","news-media-hijacking","far-right-manipulation","2016-election","mental-toughness","psychological-armor","cyber-resilience","hate-speech","digital-violence","terrorism-online","platform-design","friction-removal","usability-vs-safety","technopanic-podcast","privacy-vs-security","confucius","good-vs-evil"]}
---

# DL 190

## Hate, Fear, and Trolling

**Published**: 2019-03-23 â€¢ [[03 CREATE/ðŸ“§ Newsletter/ðŸ“§ Newsletter\|ðŸ“§ Newsletter]]

Welcome to Digitally Literate 190. Hate, fear, and trolling.

Hi all, my name is [Ian O'Byrne](https://wiobyrne.com/start/) and welcome to Digitally Literate. I'm remixing & rebranding a couple of my digital streams as I continue to think about the signals I create & consume online. As part of this, TL;DR is now Digitally Literate.

I research, teach, & write about technology in our lives. In this newsletter, I try to synthesize what happened this week so you can be digitally literate as well. This week I pull together some of the varying discussion about the role of digital, social spaces in the Christchurch mosque shootings from last week. If this content is too chilling, please feel free to skip this issue and join us next week.

I posted a couple of other things this week:

- [The Technopanic Podcast](https://screentime.me/technopanic-podcast/) - My podcast with Kristen Turner went live this week. Subscribe on [iTunes](https://itunes.apple.com/us/podcast/technopanic-podcast-living-learning-in-age-screentime/id1453320328), [Spotify](https://open.spotify.com/show/2xnLc2qVa7Q2lkYN2F0W9i), [PocketCasts](https://pca.st/OCfk), or the podcast catcher of your choice.
- [Understanding the differences between privacy and security](https://screentime.me/understanding-the-differences-between-privacy-security/) - It is your responsibility to protect and secure yourself while using digital tools and spaces. This primer gives an overview on some of the language we should use.
- [Use the Internet Archive, WordPress, & Blubrry Plugin to set up audio podcasts](https://www.youtube.com/watch?v=EL1o9dFQYdI&feature=youtu.be) - A video overview of how I used the tools and spaces listed above to host and share the Technopanic podcast.

---

## ðŸ”– Key Takeaways

- **Algorithmic Radicalization**: Online extremism operates through platform-driven pipelines that nudge users toward increasingly strident beliefs through recommendation algorithms and engagement optimization.
- **Shitposting as Weapon**: The Christchurch shooter's manifesto deliberately mixed genuine beliefs with trolling and memes to manipulate journalists into amplifying extremist messaging.
- **Platform Design Problem**: Social media's fundamental architectureâ€”designed to spread content virally without frictionâ€”makes it structurally incapable of preventing harm at scale.
- **Media Amplification Trap**: Journalism's economic and cultural pressures lead to coverage patterns that inadvertently provide oxygen to extremists seeking attention and recruitment.
- **Mental Toughness Necessity**: Building psychological resilience becomes essential survival skill for navigating online spaces weaponized for harassment and radicalization.

---

## ðŸ“º Watch

### [Christchurch Terrorism Analysis](https://www.nytimes.com/video)

The gunman in the Christchurch mosque shootings shared a racist manifesto online and posted live video of his attack on Facebook. The NY Times spoke to terrorism experts about why this matters.

The Christchurch attack represented a new synthesis of online extremism and real-world violence. The shooter didn't just commit mass murderâ€”he produced it as content, optimized for viral spread. Live-streaming on Facebook, manifesto posted to 8chan, shoutouts to YouTube personalities during the killing. The attack was designed to be shared, discussed, memed. Previous mass shooters sought media coverage; this one created his own media. The platforms faced impossible task: content spread faster than moderation could work, copies proliferated through screen recordings and re-uploads, and the viral mechanics that make platforms valuable made them complicit in terrorism. The conversation shifted from "platforms need better moderation" to "perhaps platforms shouldn't be designed this way."

---

## ðŸ“š Read

### [Online Extremism and the Radicalization Pipeline](https://www.nytimes.com)

[Kevin Roose](https://twitter.com/kevinroose) on the mass murders in Christchurch, New Zealand.

> Now, online extremism is just regular extremism on steroids. There is no offline equivalent of the experience of being algorithmically nudged toward a more strident version of your existing beliefs, or having an invisible hand steer you from gaming videos to neo-Nazism. The internet is now the place where the seeds of extremism are planted and watered, where platform incentives guide creators toward the ideological poles, and where people with hateful and violent beliefs can find and feed off one another.

Roose identifies something crucial: algorithmic radicalization has no offline analog. The recommendation engine doesn't just connect people with similar interestsâ€”it optimizes for engagement, and outrage engages. A teenager watching gaming videos gets recommended anti-feminist content, then men's rights material, then white nationalist propaganda. Each step feels natural, chosen. The platform invisibly shepherds users toward extremity because extreme content generates extreme engagement. The radicalization pipeline isn't a bug but an emergent property of engagement-optimized systems. How do we talk about this in schools? The challenge extends beyond media literacyâ€”students need to understand they're navigating systems designed to manipulate their attention and beliefs.

### [The Shooter's Manifesto Was Designed to Troll](https://www.theatlantic.com/technology/archive/2019/03/the-shooters-manifesto-was-designed-to-troll/585058/)

[Taylor Lorenz](https://twitter.com/TaylorLorenz) in The Atlantic.

> Significant portions of the manifesto appear to be an elaborate troll, written to prey on the mainstream media's worst tendencies. As the journalist Robert Evans noted, "This manifesto is a trap â€¦ laid for journalists searching for the meaning behind this horrific crime. There is truth in there, and valuable clues to the shooter's radicalization, but it is buried beneath a great deal of, for lack of a better word, 'shitposting.'"

[Shitposting](https://knowyourmeme.com/memes/shitposting) means posting trollish, ironic content designed to derail conversation or elicit reaction from those not in on the joke. The Christchurch manifesto weaponized thisâ€”mixing genuine white supremacist ideology with internet memes, fake influences (he named PewDiePie and Spyro the Dragon), and deliberately provocative statements designed to generate media coverage. The trap: journalists trying to explain the attack would inevitably amplify the message, spreading extremist content while debunking it. This represents evolved propagandaâ€”not just lying but poisoning the information environment so truth becomes impossible to extract. Media literacy now requires understanding that some content is designed specifically to be misreported.

### [Social Media Are a Mass Shooter's Best Friend](https://www.theatlantic.com/technology/archive/2019/03/how-terrorism-new-zealand-spread-social-media/585040/)

[Ian Bogost](https://twitter.com/ibogost) on how technology platforms police content.

> But the internet separates images from context and action from intention, and then it spreads those messages quickly among billions of people scattered all around the globe. The internet was designed to resist the efforts of any central authority to control its contentâ€”even when a few large, wealthy companies control the channels by which most users access information.

> It's worth remembering that "viral" spread once referred to contagious disease, not to images and ideas. As long as technology platforms drive the spread of global information, they can't help but carry it like a plague.

Bogost's analysis cuts deep: platforms can do better, but better isn't the solution. The problem is the media ecosystem itselfâ€”systems designed to spread content without friction will inevitably spread harm. The decentralized architecture that makes the internet resilient also makes it uncontrollable. The viral metaphor is literal: content spreads through networks like pathogens through populations. Platforms face structural impossibility: the features that make them valuable (rapid sharing, global reach, algorithmic amplification) are precisely the features that make them dangerous. The only solution that worksâ€”limiting usability and reachâ€”directly conflicts with business models built on maximizing engagement.

### [The Attack That Broke the Net's Safety Net](https://www.nytimes.com/2019/03/18/opinion/facebook-youtube-mass-shootings.html)

The NY Times Editorial Board on how the Christchurch attack overwhelmed content moderation systems.

> It's telling that the platforms must make themselves less functional in the interests of public safety. What happened this weekend gives an inkling of how intractable the problem may be. Internet platforms have been designed to monopolize human attention by any means necessary, and the content moderation machine is a flimsy check on a system that strives to overcome all forms of friction.

The editorial identifies the fundamental tension: platforms designed to maximize engagement have bolted on content moderation as afterthought. The systems that spread content operate at machine speed; moderation operates at human speed. The Christchurch video spawned copies faster than reviewers could remove themâ€”one upload became dozens became thousands. The "safety net" is structurally inadequate to the system it's meant to catch. The honest solutionâ€”making platforms less functional, adding friction, limiting viralityâ€”threatens profitability. So we get the worst of both worlds: platforms optimized for spread with moderation teams perpetually overwhelmed.

### [The Oxygen of Amplification](https://datasociety.net/output/oxygen-of-amplification/)

A report on better practices for reporting on extremists by [Whitney Phillips](https://twitter.com/wphillips49). This Data & Society report showcases how news media was hijacked from 2016 to 2018 to amplify hate group messages.

The report has three parts: historical overview of far-right manipulation using trolling and meme culture during 2016; consequences of reporting on problematic information and structural journalism limitations; and tactical recommendations for establishing newsworthiness and handling manipulators.

Phillips' research documents systematic exploitation of journalism norms. Extremists learned that outrageous statements guarantee coverageâ€”the more offensive, the more newsworthy. Journalists face impossible choice: ignore dangerous movements (failing public interest) or cover them (providing platform). The report advocates "better practices" over "best practices"â€”acknowledging no perfect solutions while offering tactical improvements. Key insight: newsworthiness is constructed, not given. Journalists can choose what amplifies. The oxygen metaphor: fire needs oxygen to burn, extremism needs attention to spread. Strategic silence sometimes serves public interest better than coverage.

---

## ðŸ”¨ Do

### Building Psychological Armor

Although the structures to make you safe online leave a lot to be desired, there are some things you can do to protect yourself from cyberhate. One of the first steps is to get your psychological armor on.

Dr. Sean Richardson spoke about [failure and its relation to mental toughness](https://www.youtube.com/watch?v=LCPgvTRftZg) in a TEDx talk.

Build your mental toughness. Manage your expectations. Prevent emotions from getting the best of you. Find your source of motivation. Learn to delay gratification and let things go.

Richardson's framework for mental toughness becomes survival skill for digital age. Online spaces are designed to provoke emotional reactionsâ€”outrage drives engagement, fear keeps attention. Building psychological resilience means developing capacity to encounter provocation without reactive response. This isn't about becoming numb but about choosing when and how to engage. Managing expectations acknowledges that online interactions rarely match offline norms. Finding motivation means connecting to purpose beyond winning arguments. Letting things go recognizes that some battles can't be won and shouldn't be fought. Mental toughness for the internet age means emotional regulation as practiced skill.

---

## ðŸ¤” Consider

> "It is easy to hate and it is difficult to love. This is how the whole scheme of things works. All good things are difficult to achieve; and bad things are very easy to get." â€” Confucius

Confucius captures the asymmetry that platforms exploit. Hate spreads faster than love because hatred is simplerâ€”binary, certain, energizing. Love requires complexity, nuance, patience. Platform design amplifies easy over difficult, fast over slow, reaction over reflection. The Christchurch shooter understood thisâ€”hatred is easily packaged, virally spreadable, engagement-optimized. Building counter-movements requires the harder work of connection, understanding, and sustained commitment that algorithms don't reward.

---

## ðŸ”— Navigation

**Previous**: [[03 CREATE/ðŸ“§ Newsletter/TLDR 189\|TLDR 189]] â€¢ **Next**: [[03 CREATE/ðŸ“§ Newsletter/DL 191\|DL 191]] â€¢ **Archive**: [[03 CREATE/ðŸ“§ Newsletter/ðŸ“§ Newsletter\|ðŸ“§ Newsletter]]

**ðŸŒ± Connected Concepts**:

- [[Online Extremism\|Online Extremism]] â€” Algorithmic radicalization pipelines nudging users toward increasingly strident beliefs through engagement optimization and recommendation systems in [[Platform Design\|Platform Design]].
- [[Content Moderation\|Content Moderation]] â€” Platform safety systems operating at human speed while viral spread operates at machine speed creating structural inadequacy in [[Social Media Governance\|Social Media Governance]].
- [[Media Manipulation\|Media Manipulation]] â€” Extremists exploiting journalism norms through shitposting manifestos mixing genuine ideology with trolling to guarantee amplifying coverage in [[Information Warfare\|Information Warfare]].
- [[Platform Responsibility\|Platform Responsibility]] â€” Tension between engagement-maximizing design and public safety requiring platforms to make themselves less functional in [[Tech Ethics\|Tech Ethics]].
- [[03 CREATE/ðŸŒ² Evergreens/Digital Resilience\|03 CREATE/ðŸŒ² Evergreens/Digital Resilience]] â€” Building psychological armor and mental toughness as survival skills for navigating spaces designed to provoke emotional reactions in [[03 CREATE/ðŸŒ² Evergreens/Media Literacy\|Media Literacy]].

---

*Part of the [[03 CREATE/ðŸ“§ Newsletter/ðŸ“§ Newsletter\|ðŸ“§ Newsletter]] archive documenting digital literacy and technology.*
